import openai
import time
import os
from pydantic import BaseModel
from typing import Optional, List, Any, Callable, Union
from tracer import tracer

from openai.types.chat import ChatCompletionMessage

def get_env_var(var_name: str, default: Optional[Any] = None, required: bool = True) -> Any:
    """
    Retrieves the value of an environment variable.

    Args:
        var_name (str): The name of the environment variable to retrieve.
        default (Optional[str], optional): The default value to return if the variable is not set. Defaults to None.
        required (bool, optional): Whether the variable is required. If True and the variable is not set, an exception is raised. Defaults to True.

    Returns:
        Optional[str]: The value of the environment variable, or the default value if not set.

    Raises:
        EnvironmentError: If the variable is required and not set.
    """
    value = os.getenv(var_name, default)
    if value is None:
        if required:
            raise EnvironmentError(f"Environment variable {var_name} is required but not set.")
        tracer.warning(f"Environment variable {var_name} is not set.")
    return value


class ConversationMessage(BaseModel):
    """
    Represents a single message in the conversation.

    Attributes:
        content (Optional[str]): The contents of the message.
        refusal (Optional[str]): The refusal message generated by the model.
        role (str): The role of the author of this message (e.g., 'user', 'system').
        audio (Optional[dict]): Optional audio data for the message.
        function_call (Optional[dict]): Deprecated. Function calls generated by the model.
        tool_calls (Optional[List[dict]]): Tool calls generated by the model.
        tool_call_id (Optional[str]): The ID of the tool call.
    """
    content: Optional[str] = None
    refusal: Optional[str] = None
    role: str
    audio: Optional[dict] = None
    function_call: Optional[dict] = None
    tool_calls: Optional[List[dict]] = None
    tool_call_id: Optional[str] = None


class ChatGPTRequester:
    def __init__(
        self, 
        client=None,
        model: str = "gpt-4o",
        embeddings_model: str = "text-embedding-ada-002",
        retries: int = 10,
        service_unavailable_wait_secs: float = 0.5,
        temperature: float = 0.0,
    ) -> None:
        self._client = client or openai.OpenAI(
            api_key=get_env_var("OPENAI_API_KEY")
        )
        self._model = model
        self._embeddings_model = embeddings_model
        self._retries = retries
        self._service_unavailable_wait_secs = service_unavailable_wait_secs
        self._temperature = temperature

        # Token counters
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0

    def reset_token_counters(self) -> None:
        """Reset the token counters to zero."""
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0

    def _retry_on_failure(self, func: Callable, *args, **kwargs) -> Optional[Any]:
        """
        Executes a function with retries in case of failures.

        Args:
            func (Callable): The function to execute.
            *args: Positional arguments for the function.
            **kwargs: Keyword arguments for the function.

        Returns:
            Optional[Any]: The result of the function if successful, or None if retries are exhausted.
        """
        retries = self._retries
        wait_time = self._service_unavailable_wait_secs
        while retries > 0:
            try:
                return func(*args, **kwargs)
            except openai.RateLimitError:
                tracer.info(f"Rate limit exceeded. Retrying in {wait_time} seconds...")
                time.sleep(self._service_unavailable_wait_secs)
                wait_time = min(wait_time * 2, 60)
            except openai.OpenAIError as e:
                tracer.error(f"OpenAI error: {e}. Retries left: {retries}.")
                return None
            except Exception as e:
                tracer.error(f"Unexpected error: {e}. Retries left: {retries}.")
                return None
            retries -= 1
        tracer.error("Failed after maximum retries.")
        return None

    def request(self, messages: List[Union[ChatCompletionMessage, ConversationMessage]]) -> ChatCompletionMessage:
        """
        Make a request to the ChatGPT model and get a response.

        Args:
            messages (List[]): A list of messages to send to the ChatGPT model. Pueden ser ChatCompletionMessage o ConversationMessage

        Returns:
            str: The response from the ChatGPT model.
        """
        def call_openai():
            return self._client.chat.completions.create(
                model=self._model,
                messages=messages, 
                temperature=self._temperature,
            )

        response = self._retry_on_failure(call_openai)
        if response:
            tokens = response.usage
            self.prompt_tokens += tokens.prompt_tokens
            self.completion_tokens += tokens.completion_tokens
            self.total_tokens += tokens.total_tokens

            if response.choices[0].finish_reason == 'length':
                raise RuntimeError("GPT truncated output")

            return response.choices[0].message
        return "Error: Failed to get a response"

    def request_embedding(self, text: str) -> Optional[list]:
        """
        Request embedding for the given text.

        Args:
            text (str): The input text to be embedded.

        Returns:
            Optional[list]: The embedding result as a list, or None if an error occurs.
        """
        text = text.replace("\n", " ")

        def call_openai():
            return self._client.embeddings.create(
                input=[text],
                model=self._embeddings_model
            )

        embedding_result = self._retry_on_failure(call_openai)
        if embedding_result:
            tokens = embedding_result.usage
            self.prompt_tokens += tokens.prompt_tokens
            self.completion_tokens += tokens.completion_tokens
            self.total_tokens += tokens.total_tokens
            return embedding_result.data[0].embedding

        return None
