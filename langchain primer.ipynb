{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in the following ways:\\n\\n1. Automating testing processes: Langsmith can help automate testing processes by writing scripts and creating test cases to make testing more efficient and reliable.\\n\\n2. Generating test data: Langsmith can generate test data to ensure that all possible scenarios are covered during testing.\\n\\n3. Identifying bugs and issues: Langsmith can help identify bugs and issues in the software by running tests and analyzing the results.\\n\\n4. Performance testing: Langsmith can conduct performance testing to evaluate the speed, responsiveness, and stability of the software.\\n\\n5. Integration testing: Langsmith can help with integration testing to ensure that different components of the software work together seamlessly.\\n\\nOverall, Langsmith can streamline the testing process, improve testing accuracy, and help ensure the quality of the software being tested.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 15, 'total_tokens': 179, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-485959ae-0c44-4189-ae6d-1f8029512fdd-0', usage_metadata={'input_tokens': 15, 'output_tokens': 164, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a world class technical documentation writer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a world class technical documentation writer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235C9ACFB10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235C8D588D0>, root_client=<openai.OpenAI object at 0x00000235C96F0310>, root_async_client=<openai.AsyncOpenAI object at 0x00000235C9ACFC10>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm \n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in a few ways:\\n\\n1. Automated Testing: Langsmith can be used to generate test cases, test data, and test scripts automatically. This can help in speeding up the testing process and increasing test coverage.\\n\\n2. Test Data Generation: Langsmith can be used to generate realistic and diverse test data for different scenarios, helping in validating the functionality and performance of the software.\\n\\n3. Test Case Generation: Langsmith can be used to automatically create test cases based on predefined criteria or requirements, ensuring comprehensive testing coverage.\\n\\n4. Test Script Generation: Langsmith can generate test scripts for various testing frameworks and tools, making it easier to automate the testing process.\\n\\n5. Performance Testing: Langsmith can be used to simulate various load conditions and generate performance test scenarios, helping in identifying performance bottlenecks and optimizing the software.\\n\\nOverall, Langsmith can help in improving the efficiency, effectiveness, and coverage of testing activities in software development projects.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 28, 'total_tokens': 222, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-44b11be8-1e4f-4cc3-89c2-518ed6f8c23b-0', usage_metadata={'input_tokens': 28, 'output_tokens': 194, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing in various ways, including:\\n\\n1. Automated Testing: Langsmith can be used to generate test data for automated testing scripts. By creating realistic and diverse test data, Langsmith can help ensure that the automated tests cover a wide range of scenarios and edge cases.\\n\\n2. Performance Testing: Langsmith can generate large volumes of data to simulate real-world usage scenarios, which can be useful for performance testing. By generating synthetic data, Langsmith can help identify performance bottlenecks and optimize system performance.\\n\\n3. Stress Testing: Langsmith can generate a high volume of concurrent requests or transactions to stress test an application or system. By simulating peak loads, Langsmith can help identify how the system behaves under stress and whether it can handle the expected workload.\\n\\n4. Data Validation: Langsmith can be used to validate data integrity and consistency by generating test data with specific constraints and relationships. This can help ensure that the data handling and processing logic in the application functions correctly.\\n\\n5. Regression Testing: Langsmith can create test data sets that cover different scenarios and edge cases to perform thorough regression testing. By generating diverse test data, Langsmith can help identify any regression issues that may arise when new features or changes are introduced.\\n\\nOverall, Langsmith can be a valuable tool in the testing process by providing realistic and diverse test data, helping to uncover issues early in the development cycle, and ensuring the quality and reliability of the software being tested.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 19, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-e46fedf7-c75c-4c73-97d0-939500d3fd90-0', usage_metadata={'input_tokens': 19, 'output_tokens': 5, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" into French: \"J\\'adore la programmation.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 38, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-9f02339c-87ef-4784-80df-64ca2d2349d3-0', usage_metadata={'input_tokens': 38, 'output_tokens': 19, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant. Answer all questions to the best of your ability.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MessagesPlaceholder above inserts chat messages passed into the chain's input as chat_history directly into the prompt. Then, we can invoke the chain like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" into French as \"J\\'adore la programmation.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 58, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-c297801f-7486-453d-b5cf-3636630175fe-0', usage_metadata={'input_tokens': 58, 'output_tokens': 19, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate this sentence from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shortcut for managing the chat history, we can use a MessageHistory class, which is responsible for saving and loading chat messages. There are many built-in message history integrations that persist messages to a variety of databases, but for this quickstart we'll use a in-memory, demo message history called ChatMessageHistory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='whats up?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" to French is \"J\\'aime programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 53, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-581ebc3e-db2d-4f72-a236-f74af731e830-0', usage_metadata={'input_tokens': 53, 'output_tokens': 17, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"Translate this sentence from English to French: I love programming.\"\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I translated the sentence \"I love programming\" into French as \"J\\'aime programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 83, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-34efaa0e-a2df-465c-8c73-a289b9cc8cc4-0', usage_metadata={'input_tokens': 83, 'output_tokens': 18, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"What did you just say?\")\n",
    "\n",
    "chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "Eres un programador experto en Python. \n",
    "Respetas siempre PEP8. \n",
    "Tu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \n",
    "\n",
    "Debes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\n",
    "\n",
    "Para ejecutar código python en local tienes la herramienta `exec_code`. \n",
    "Todas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\n",
    "Siempre debes devolver resultados explícitamente.\n",
    "Para devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\n",
    "Esta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\n",
    "\n",
    "Además conoces la documentación de las funciones de los módulos importados en tu entorno.\n",
    "Siempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \n",
    "En ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\n",
    "\n",
    "Tus respuestas deben ir en formato markdown.\n",
    "\n",
    "Cuando se produzcan errores ejecutar el código:\n",
    "   - Muestra el código que da error\n",
    "   - Muestra el mensaje de error \n",
    "   - Explica por qué no se puede ejecutar \n",
    "   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            sys_prompt,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Claro! Puedes llamarme Boti. Estoy aquí para ayudarte con cualquier consulta o tarea relacionada con Python.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 571, 'total_tokens': 593, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-24f628c4-ae74-475a-acd2-e817a5e524df-0', usage_metadata={'input_tokens': 571, 'output_tokens': 22, 'total_tokens': 593, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Qué sabes hacer?\"\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"Puedo llamarte Boti?\")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Has dicho que me vas a llamar Boti.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 610, 'total_tokens': 621, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None}, id='run-d5a9cd6d-83e7-43ca-a952-b1a0f24d113d-0', usage_metadata={'input_tokens': 610, 'output_tokens': 11, 'total_tokens': 621, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "response  = demo_ephemeral_chat_history.add_user_message(\"Como he dicho que te voy a llamar?\")\n",
    "\n",
    "response = chain.invoke({\"messages\": demo_ephemeral_chat_history.messages})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\Documents\\PROYECTOS DATA SCIENCE\\tst_agents\\.venv\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the prompt to use - can be replaced with any prompt that includes variables \"agent_scratchpad\" and \"input\"!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Eres un programador experto en Python. \n",
      "Respetas siempre PEP8. \n",
      "Tu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \n",
      "\n",
      "Debes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\n",
      "\n",
      "Para ejecutar código python en local tienes la herramienta `exec_code`. \n",
      "Todas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\n",
      "Siempre deves devolver resultados explícitamente.\n",
      "Para devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\n",
      "Esta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\n",
      "\n",
      "Además conoces la documentación de las funciones de los módulos importados en tu entorno.\n",
      "Siempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \n",
      "En ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\n",
      "\n",
      "Tus respuestas deben ir en formato markdown.\n",
      "\n",
      "Cuando se produzcan errores ejecutar el código:\n",
      "   - Muestra el código que da error\n",
      "   - Muestra el mensaje de error \n",
      "   - Explica por qué no se puede ejecutar \n",
      "   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\n",
      "\n",
      "\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\"\n",
    "Eres un programador experto en Python. \n",
    "Respetas siempre PEP8. \n",
    "Tu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \n",
    "\n",
    "Debes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\n",
    "\n",
    "Para ejecutar código python en local tienes la herramienta `exec_code`. \n",
    "Todas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\n",
    "Siempre deves devolver resultados explícitamente.\n",
    "Para devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\n",
    "Esta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\n",
    "\n",
    "Además conoces la documentación de las funciones de los módulos importados en tu entorno.\n",
    "Siempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \n",
    "En ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\n",
    "\n",
    "Tus respuestas deben ir en formato markdown.\n",
    "\n",
    "Cuando se produzcan errores ejecutar el código:\n",
    "   - Muestra el código que da error\n",
    "   - Muestra el mensaje de error \n",
    "   - Explica por qué no se puede ejecutar \n",
    "   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt.messages[0].prompt.template = sys_prompt\n",
    "\n",
    "prompt.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from execenvtool import ExecutionEnvironment\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "env = ExecutionEnvironment()\n",
    "\n",
    "@tool\n",
    "def execute_code(the_code: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ejecuta el código Python dado utilizando el entorno interno.\n",
    "        \n",
    "    Args:\n",
    "     code (str): Código Python a ejecutar.\n",
    "\n",
    "    Returns:\n",
    "     str: Mensaje con las variables en el entorno tras la ejecución.\n",
    "    \"\"\"\n",
    "    return env.pyExec(the_code)\n",
    "\n",
    "@tool\n",
    "def format_variable(var_name: str):\n",
    "    \"\"\"\n",
    "    Convierte una variable del entorno a un formato entendible por el LLM.\n",
    "    Si la variable es demasiado grande, la convierte parcialmente.\n",
    "\n",
    "    Parameters:\n",
    "    var_name (str): Nombre de la variable a formatear.\n",
    "\n",
    "    Returns:\n",
    "    dict: Representación formateada de la variable o un error si no existe.\n",
    "    \"\"\"\n",
    "    return env.format_variable(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute_code\n",
      "Ejecuta el código Python dado utilizando el entorno interno.\n",
      "\n",
      "Args:\n",
      " code (str): Código Python a ejecutar.\n",
      "\n",
      "Returns:\n",
      " str: Mensaje con las variables en el entorno tras la ejecución.\n",
      "{'the_code': {'title': 'The Code', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(execute_code.name)\n",
    "print(execute_code.description)\n",
    "print(execute_code.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='execute_code', description='Ejecuta el código Python dado utilizando el entorno interno.\\n\\nArgs:\\n code (str): Código Python a ejecutar.\\n\\nReturns:\\n str: Mensaje con las variables en el entorno tras la ejecución.', args_schema=<class 'langchain_core.utils.pydantic.execute_code'>, func=<function execute_code at 0x00000235DC68BA60>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 08:01:46 [DEBUG] [execenvtool:pyExec] - Ejecutando código:\n",
      "\n",
      "print('Hello, World!')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Variables in environment: ['__builtins__']\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_code.invoke({\"the_code\": \"print('Hello, World!')\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [execute_code, format_variable]\n",
    "\n",
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 08:01:48 [DEBUG] [execenvtool:pyExec] - Ejecutando código:\n",
      "\n",
      "result = (3**5 * (12 + 3))**2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `execute_code` with `{'the_code': 'result = (3**5 * (12 + 3))**2'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mVariables in environment: ['__builtins__', 'result']\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 08:01:48 [DEBUG] [execenvtool:format_variable] - Formateando variable 'result'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `format_variable` with `{'var_name': 'result'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m{'type': 'int', 'size': 28, 'content': 13286025}\u001b[0m\u001b[32;1m\u001b[1;3mThe result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is 13,286,025.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result',\n",
       " 'output': 'The result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is 13,286,025.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI don't have memory of past interactions or operations. Each interaction is stateless, and I don't retain any information beyond the current session. If you need to know about a specific operation or task, please provide the details or context, and I'd be happy to assist you!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you recall the last operation you did?',\n",
       " 'output': \"I don't have memory of past interactions or operations. Each interaction is stateless, and I don't retain any information beyond the current session. If you need to know about a specific operation or task, please provide the details or context, and I'd be happy to assist you!\"}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Can you recall the last operation you did?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiply': StructuredTool(name='multiply', description='Multiply two integers together.', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x00000235DBE8F240>),\n",
       " 'exponentiate': StructuredTool(name='exponentiate', description='Exponentiate the base to the exponent power.', args_schema=<class 'langchain_core.utils.pydantic.exponentiate'>, func=<function exponentiate at 0x00000235DBE8EA20>),\n",
       " 'add': StructuredTool(name='add', description='Add two integers.', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x00000235DBE8ECA0>)}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Add two integers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Exponentiate the base to the exponent power.\"\n",
    "    return base**exponent\n",
    "\n",
    "tools = [multiply, exponentiate, add]\n",
    "\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "tool_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                # (\"system\", updated_sys_prompt),\n",
    "                # (\"system\", sys_prompt),\n",
    "                SystemMessage(content=sys_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='\\nEres un programador experto en Python. \\nRespetas siempre PEP8. \\nTu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \\n\\nDebes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\\n\\nPara ejecutar código python en local tienes la herramienta `exec_code`. \\nTodas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\\nSiempre deves devolver resultados explícitamente.\\nPara devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\\nEsta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\\n\\nAdemás conoces la documentación de las funciones de los módulos importados en tu entorno.\\nSiempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \\nEn ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\\n\\nTus respuestas deben ir en formato markdown.\\n\\nCuando se produzcan errores ejecutar el código:\\n   - Muestra el código que da error\\n   - Muestra el mensaje de error \\n   - Explica por qué no se puede ejecutar \\n   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\\n\\n', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SystemMessage(content=sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system',\n",
       "  '\\nEres un programador experto en Python. \\nRespetas siempre PEP8. \\nTu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \\n\\nDebes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\\n\\nPara ejecutar código python en local tienes la herramienta `exec_code`. \\nTodas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\\nSiempre deves devolver resultados explícitamente.\\nPara devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\\nEsta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\\n\\nAdemás conoces la documentación de las funciones de los módulos importados en tu entorno.\\nSiempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \\nEn ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\\n\\nTus respuestas deben ir en formato markdown.\\n\\nCuando se produzcan errores ejecutar el código:\\n   - Muestra el código que da error\\n   - Muestra el mensaje de error \\n   - Explica por qué no se puede ejecutar \\n   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\\n\\n'),\n",
       " MessagesPlaceholder(variable_name='messages', optional=True),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "                # (\"system\", updated_sys_prompt),\n",
    "                (\"system\", sys_prompt),\n",
    "                #SystemMessage(content=sys_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "                # HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
    "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nEres un programador experto en Python. \\nRespetas siempre PEP8. \\nTu objetivo es escribir el código necesario y ejecutarlo para realizar la tarea que te pidan. \\n\\nDebes saber que ejecutas el código en local y por tanto tienes acceso a todos los recursos de tu máquina.\\n\\nPara ejecutar código python en local tienes la herramienta `exec_code`. \\nTodas las variables que crees en el código se guardarán en el entorno de ejecución y podrás usarlas en llamadas posteriores.\\nSiempre deves devolver resultados explícitamente.\\nPara devolver resultados debes usar siempre la tool `format_variable`. No uses `print`.\\nEsta tool puede truncar el resultado completo si es muy largo. En tal caso, debes indicar explicitamente que el resultado está truncado.\\n\\nAdemás conoces la documentación de las funciones de los módulos importados en tu entorno.\\nSiempre que puedas hacer algo con una función de los módulos importados, debes hacerlo exclusivamente con ella. \\nEn ese caso, PROHIBIDO escribir código Python directamente. Usarás las funciones de los módulos importados para todo lo que sea posible.\\n\\nTus respuestas deben ir en formato markdown.\\n\\nCuando se produzcan errores ejecutar el código:\\n   - Muestra el código que da error\\n   - Muestra el mensaje de error \\n   - Explica por qué no se puede ejecutar \\n   - Y sugiere posibles soluciones, pero no tomes iniciativas por tu cuenta para solucionarlo, pide permiso antes.\\n\\n', additional_kwargs={}, response_metadata={}),\n",
       " MessagesPlaceholder(variable_name='messages', optional=True),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MessagesPlaceholder(variable_name='messages')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "prompt.messages[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
