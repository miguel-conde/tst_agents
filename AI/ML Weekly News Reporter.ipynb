{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AInsight: AI/ML Weekly News Reporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Overview\n",
    "This notebook demonstrates the implementation of an intelligent news aggregation and summarization system using a multi-agent architecture. AInsight automatically collects, processes, and summarizes AI/ML news for general audiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The rapid evolution of AI/ML technology creates several challenges:\n",
    "- Information overload from multiple news sources\n",
    "- Technical complexity making news inaccessible to general audiences\n",
    "- Time-consuming manual curation and summarization\n",
    "- Inconsistent reporting formats and quality\n",
    "\n",
    "AInsight addresses these challenges through:\n",
    "- Automated news collection and filtering\n",
    "- Intelligent summarization for non-technical readers\n",
    "- Consistent, well-structured reporting\n",
    "- Scalable, maintainable architecture\n",
    "- Saving time and effort!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Multi-Agent System Architecture\n",
    "\n",
    "AInsight processes news through three specialized agents:\n",
    "\n",
    "1. **NewsSearcher Agent**\n",
    "   - Primary news collection engine\n",
    "   - Interfaces with Tavily API\n",
    "   - Filters for relevance and recency\n",
    "   - Handles source diversity\n",
    "\n",
    "2. **Summarizer Agent**\n",
    "   - Processes technical content\n",
    "   - Uses gpt-4o-mini for natural language generation (LLM can be configured per user preference, used OpenAI in this tutorial for accessibility)\n",
    "   - Handles technical term simplification\n",
    "\n",
    "3. **Publisher Agent**\n",
    "   - Takes list of summaries as input\n",
    "   - Formats them into a structured prompt\n",
    "   - Makes single gpt-4o-mini call to generate complete report with:\n",
    "     * Introduction section\n",
    "     * Organized summaries\n",
    "     * Further reading links\n",
    "   - Saves final report as markdown file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/ainsight_langgraph.svg\" alt=\"ainsight by langgraph\" style=\"width:20%; height:50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Learning Objectives\n",
    "1. Understand multi-agent system architecture\n",
    "2. Implement state management with LangGraph\n",
    "3. Work with external APIs (Tavily, OpenAI)\n",
    "4. Create modular, maintainable Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Technical Requirements\n",
    "- Python 3.11+\n",
    "- OpenAI API key\n",
    "- Tavily API key\n",
    "- Required packages (see setup section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Configuration\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (0.3.11)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: langgraph in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (0.2.59)\n",
      "Requirement already satisfied: tavily-python in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain-openai) (1.57.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.4 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langgraph) (2.0.9)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langgraph) (0.1.45)\n",
      "Requirement already satisfied: httpx in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
      "Requirement already satisfied: anyio in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from httpx->tavily-python) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from httpx->tavily-python) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from httpx->tavily-python) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from httpx->tavily-python) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.55.3->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\migue\\documents\\proyectos data science\\tst_agents\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.55.3->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-openai langgraph tavily-python python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "Create a `.env` file in your project directory with the following:\n",
    "\n",
    "```plaintext\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "TAVILY_API_KEY=your-tavily-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "from typing import Dict, List, Any, TypedDict, Optional\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize API clients\n",
    "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Models and State Management\n",
    "\n",
    "Think of state as a \"memory\" that will flow through your workflow (graph) later.\n",
    "\n",
    "We use Pydantic and TypedDict to define our data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single news article\n",
    "    \n",
    "    Attributes:\n",
    "        title (str): Article headline\n",
    "        url (str): Source URL\n",
    "        content (str): Article content\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "class Summary(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents a processed article summary\n",
    "    \n",
    "    Attributes:\n",
    "        title (str): Original article title\n",
    "        summary (str): Generated summary\n",
    "        url (str): Source URL for reference\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    summary: str\n",
    "    url: str\n",
    "\n",
    "# This defines what information we can store and pass between nodes later\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Maintains workflow state between agents\n",
    "    \n",
    "    Attributes:\n",
    "        articles (Optional[List[Article]]): Found articles\n",
    "        summaries (Optional[List[Summary]]): Generated summaries\n",
    "        report (Optional[str]): Final compiled report\n",
    "    \"\"\"\n",
    "    articles: Optional[List[Article]] \n",
    "    summaries: Optional[List[Summary]] \n",
    "    report: Optional[str] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Agent Implementation\n",
    "\n",
    "### 1. NewsSearcher Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSearcher:\n",
    "    \"\"\"\n",
    "    Agent responsible for finding relevant AI/ML news articles\n",
    "    using the Tavily search API\n",
    "    \"\"\"\n",
    "    \n",
    "    def search(self) -> List[Article]:\n",
    "        \"\"\"\n",
    "        Performs news search with configured parameters\n",
    "        \n",
    "        Returns:\n",
    "            List[Article]: Collection of found articles\n",
    "        \"\"\"\n",
    "        response = tavily.search(\n",
    "            query=\"artificial intelligence and machine learning news\", \n",
    "            topic=\"news\",\n",
    "            time_period=\"1m\",\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=10\n",
    "        )\n",
    "        \n",
    "        articles = []\n",
    "        for result in response['results']:\n",
    "            articles.append(Article(\n",
    "                title=result['title'],\n",
    "                url=result['url'],\n",
    "                content=result['content']\n",
    "            ))\n",
    "        \n",
    "        return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    \"\"\"\n",
    "    Agent that processes articles and generates accessible summaries\n",
    "    using gpt-4o-mini\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI expert who makes complex topics accessible \n",
    "        to general audiences. Summarize this article in 2-3 sentences, focusing on the key points \n",
    "        and explaining any technical terms simply.\n",
    "        \"\"\"\n",
    "    \n",
    "    def summarize(self, article: Article) -> str:\n",
    "        \"\"\"\n",
    "        Generates an accessible summary of a single article\n",
    "        \n",
    "        Args:\n",
    "            article (Article): Article to summarize\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated summary\n",
    "        \"\"\"\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Title: {article.title}\\n\\nContent: {article.content}\")\n",
    "        ])\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Publisher Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Publisher:\n",
    "    \"\"\"\n",
    "    Agent that compiles summaries into a formatted report \n",
    "    and saves it to disk\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_report(self, summaries: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Creates and saves a formatted markdown report\n",
    "        \n",
    "        Args:\n",
    "            summaries (List[Dict]): Collection of article summaries\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated report content\n",
    "        \"\"\"\n",
    "        prompt = \"\"\"\n",
    "        Create a weekly AI/ML news report for the general public. \n",
    "        Format it with:\n",
    "        1. A brief introduction\n",
    "        2. The main news items with their summaries\n",
    "        3. Links for further reading\n",
    "        \n",
    "        Make it engaging and accessible to non-technical readers.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Format summaries for the LLM\n",
    "        summaries_text = \"\\n\\n\".join([\n",
    "            f\"Title: {item['title']}\\nSummary: {item['summary']}\\nSource: {item['url']}\"\n",
    "            for item in summaries\n",
    "        ])\n",
    "        \n",
    "        # Generate report\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=prompt),\n",
    "            HumanMessage(content=summaries_text)\n",
    "        ])\n",
    "        \n",
    "        # Add metadata and save\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        markdown_content = f\"\"\"\n",
    "        Generated on: {current_date}\n",
    "\n",
    "        {response.content}\n",
    "        \"\"\"\n",
    "        \n",
    "        filename = f\"ai_news_report_{current_date}.md\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(markdown_content)\n",
    "        \n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Workflow Implementation\n",
    "\n",
    "### State Management Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of nodes as the \"workers\" (aka agents) in your workflow. Each node:\n",
    "\n",
    "1. Takes the current state\n",
    "2. Processes it\n",
    "3. Returns updated state\n",
    "\n",
    "For example the node of NewsSearcher agent:  \n",
    "\n",
    "1. Takes current state (empty at first)\n",
    "2. Searches for articles\n",
    "3. Updates state with found articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for article search\n",
    "    \n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with found articles\n",
    "    \"\"\"\n",
    "    searcher = NewsSearcher()\n",
    "    state['articles'] = searcher.search() \n",
    "    return state\n",
    "\n",
    "def summarize_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for article summarization\n",
    "    \n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with summaries\n",
    "    \"\"\"\n",
    "    summarizer = Summarizer()\n",
    "    state['summaries'] = []\n",
    "    \n",
    "    for article in state['articles']: # Uses articles from previous node\n",
    "        summary = summarizer.summarize(article)\n",
    "        state['summaries'].append({\n",
    "            'title': article.title,\n",
    "            'summary': summary,\n",
    "            'url': article.url\n",
    "        })\n",
    "    return state\n",
    "\n",
    "def publish_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for report generation\n",
    "    \n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with final report\n",
    "    \"\"\"\n",
    "    publisher = Publisher()\n",
    "    report_content = publisher.create_report(state['summaries'])\n",
    "    state['report'] = report_content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Constructs and configures the workflow graph\n",
    "    search -> summarize -> publish\n",
    "    \n",
    "    Returns:\n",
    "        StateGraph: Compiled workflow ready for execution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a workflow (graph) initialized with our state schema\n",
    "    workflow = StateGraph(state_schema=GraphState)\n",
    "    \n",
    "    # Add processing nodes that we will flow between\n",
    "    workflow.add_node(\"search\", search_node)\n",
    "    workflow.add_node(\"summarize\", summarize_node)\n",
    "    workflow.add_node(\"publish\", publish_node)\n",
    "    \n",
    "    # Define the flow with edges\n",
    "    workflow.add_edge(\"search\", \"summarize\") # search results flow to summarizer\n",
    "    workflow.add_edge(\"summarize\", \"publish\") # summaries flow to publisher\n",
    "    \n",
    "    # Set where to start\n",
    "    workflow.set_entry_point(\"search\")\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AI/ML Weekly News Report ===\n",
      "\n",
      "# Weekly AI/ML News Report\n",
      "\n",
      "Welcome to this week's AI and Machine Learning news roundup! As technology continues to evolve, artificial intelligence is making waves across various industries, from agriculture to logistics. Here‚Äôs a look at some of the most exciting developments that are shaping our world today.\n",
      "\n",
      "## Main News Items\n",
      "\n",
      "### 1. Meet SwagBot: Revolutionizing Cattle Farming in Australia!\n",
      "Researchers at the University of Sydney have introduced SwagBot, an autonomous robot designed to enhance cattle farming. Acting like a \"smart cow,\" SwagBot uses AI to analyze data and make real-time decisions, such as guiding cattle to better grazing areas. This innovation is set to improve farming efficiency and is a sign of the growing investment in agricultural technology.\n",
      "[Read more here](https://macholevante.com/uncategorized-en/25662/meet-swagbot-revolutionizing-cattle-farming-in-australia/)\n",
      "\n",
      "### 2. What‚Äôs Your Trucking AIQ?\n",
      "The trucking industry is rapidly adopting AI technologies, especially in the wake of the pandemic. Despite facing a freight recession, AI is helping trucking fleets improve efficiency and safety through better decision-making tools. These advancements are crucial for enhancing the effectiveness of goods transportation.\n",
      "[Read more here](https://www.fleetowner.com/perspectives/lane-shift-ahead/blog/55242761/is-2025-the-year-ai-takes-the-wheel-in-trucking-spoiler-not-yet-but)\n",
      "\n",
      "### 3. AI in Agriculture Shines at Smart India Hackathon\n",
      "Innovators at the Smart India Hackathon 2024 showcased AI and ML tools aimed at improving agriculture. These technologies help monitor crop health and optimize supply chains, providing valuable insights for farmers and retailers alike. The goal is to create adaptable products that cater to various farming communities.\n",
      "[Read more here](https://www.deccanchronicle.com/news/innovators-use-ai-ml-to-boost-agriculture-1846249)\n",
      "\n",
      "### 4. Innovators Turn to AI and ML to Transform Agriculture\n",
      "The Smart India Hackathon also highlighted young innovators developing AI solutions to tackle agricultural challenges. These technologies aim to empower farmers by streamlining inventory management and reducing reliance on middlemen, promoting equal opportunities in the farming sector.\n",
      "[Read more here](https://www.deccanchronicle.com/southern-states/telangana/innovators-turn-to-ai-and-ml-to-transform-agriculture-1846035)\n",
      "\n",
      "### 5. Mana.bio Presents AI-Driven Improvements in Genetic Therapy Delivery\n",
      "Biotech startup Mana.bio has made strides in creating lipid nanoparticles for genetic therapies using AI. Their new safety model enhances the tolerability of these nanoparticles, allowing for more effective targeting of specific areas in the body, such as the lungs. This innovation could accelerate the development of medical therapies.\n",
      "[Read more here](https://www.biospace.com/press-releases/mana-bio-presents-broad-improvements-to-systemic\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run workflow\n",
    "    workflow = create_workflow()\n",
    "    final_state = workflow.invoke({\n",
    "        \"articles\": None,\n",
    "        \"summaries\": None,\n",
    "        \"report\": None\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n=== AI/ML Weekly News Report ===\\n\")\n",
    "    print(final_state['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rendering graph: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\n",
      "\n",
      "For further information visit https://errors.pydantic.dev/2.10/u/typed-dict-version\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(workflow.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Error rendering graph:\", e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Customization Options\n",
    "\n",
    "1. Modify search parameters in `NewsSearcher`:\n",
    "   - `search_depth`: \"basic\" or \"advanced\"\n",
    "   - `max_results`: Number of articles to fetch\n",
    "   - `time_period`: \"1d\", \"1w\", \"1m\", etc.\n",
    "\n",
    "2. Adjust summarization in `Summarizer`:\n",
    "   - Update `system_prompt` for different summary styles\n",
    "   - Modify GPT model parameters (temperature, max_tokens)\n",
    "\n",
    "3. Customize report format in `Publisher`:\n",
    "   - Edit the report prompt for different layouts\n",
    "   - Modify markdown template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Additional Considerations\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "1. **Content Access**\n",
    "   - Limited to publicly available news\n",
    "   - Dependency on Tavily API coverage\n",
    "   - No access to paywalled content\n",
    "\n",
    "2. **Language Support**\n",
    "   - Primary focus on English content\n",
    "\n",
    "3. **Technical Constraints**\n",
    "   - API rate limits\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "1. **Enhanced News Collection**\n",
    "   - Specify domains to search on depending on user preference\n",
    "\n",
    "2. **Improved Summarization**\n",
    "   - Add multi-language support\n",
    "   - Implement fact-checking\n",
    "\n",
    "3. **Advanced Features**\n",
    "   - Topic classification\n",
    "   - Trend detection\n",
    "\n",
    "### Specific Use Cases\n",
    "\n",
    "1. **Research Organizations**\n",
    "   - Track technology developments\n",
    "   - Monitor competition\n",
    "   - Identify collaboration opportunities\n",
    "\n",
    "2. **Educational Institutions**\n",
    "   - Create teaching materials\n",
    "   - Support student research\n",
    "   - Track field developments\n",
    "\n",
    "3. **Tech Companies**\n",
    "   - Market intelligence\n",
    "   - Innovation tracking\n",
    "   - Strategic planning\n",
    "\n",
    "4. **Media Organizations**\n",
    "   - Content curation\n",
    "   - Story research\n",
    "   - Trend analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Troubleshooting\n",
    "\n",
    "1. API Key Issues:\n",
    "   - Ensure `.env` file exists and contains valid keys\n",
    "   - Check API key permissions and quotas\n",
    "\n",
    "2. Package Dependencies:\n",
    "   - Run `pip list` to verify installations\n",
    "   - Check package versions for compatibility\n",
    "\n",
    "3. Rate Limits:\n",
    "   - Monitor API usage\n",
    "   - Implement retry logic if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Tavily search API doc: https://docs.tavily.com/docs/rest-api/api-reference\n",
    "- LangGraph Conceptual Guides: https://langchain-ai.github.io/langgraph/concepts/low_level/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
